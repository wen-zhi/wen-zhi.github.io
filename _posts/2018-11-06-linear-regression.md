---
layout: post
title: 从求解线性方程组的角度理解线性回归模型
mathjax: true
comments: true
excerpt: >-
  线性回归模型是最基础的统计学习方法之一。它的形式简单、可解释强，可能是被研究得最为深入透彻的统计模型。本文从求解非齐次线性方程组的角度来分析线性回归模型，发现了很多有趣的东西。
---

线性回归模型的本质就是**求解一个非齐次线性方程组** $X\theta = y$。

最理想的情况莫过于，$X$ 是方阵且可逆，此时有唯一解：$\theta = X^{-1}y$。
然而实际中 $X$ 常常并非方阵，或者是方阵但不可逆，此时则可能有**无穷解**或**无解**。

**无穷解**时说明存在自由变量 (free variables)，自由变量越多（即 $X$ 的秩越小），模型则越简单，因为此时它所以依赖的基变量 (base variables) 越少。这时常用的求解方法是高斯消元法（也就是进行初等变换）。

**无解**时说明 $y$ 不在 $X$ 的向量空间上（这里的向量空间指 $X$ 的列空间），此时我们想得到一个近似解，最直接的想法自然是将 $y$ 投影到 $X$ 的向量空间中，得到 $y$ 在此向量空间中的近似表示。其数学依据在于，这种投影可以使得残差 $$\|X\theta - y\|$$ 最小。此时问题就变成求解最优化问题：

$$\operatorname*{minimize}_{\theta} \|X\theta - y\|^2$$

该问题便是经典的**最小二乘问题**。对 $\theta$ 求导然后置 $0$，即可得到最优解：$\theta = (X^T X)^{-1} X^Ty$。当然这种解法同样也可以用来处理**有解**或**无穷解**时的情况，此时残差为 $0$。

不过出于数值精度上的考虑，一般不推荐计算矩阵的逆（上述解法中需要对 $X^T X$ 求逆）。因此实际中常常使用其它方法来求解线性方程组。比如，梯度下降...